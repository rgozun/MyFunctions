# Load Datafile Including Headers
def DatasetLoad(filepath,filename,target,index):
    # Load data
    filepath = filepath
    filename = filename
    df = pd.read_csv(filepath+filename,header=0)

    # Remove Duplicated Rows
    df = df.drop_duplicates(keep='first')

    # Sort Columns in Alphabetical Order
    df = df.reindex(sorted(df.columns),axis=1)

    # Move Target To Front of Dataframe
    target = target
    targetarray = df[target]
    df = df.drop(columns=target)
    df.insert(0,str(target),targetarray)

    # Set Index
    df = df.set_index(index)
    
    return(df)
    
def Metadata(df_input,AutoClass,nUniqLim):
    ColDtypes = []
    
    # Identify Datatype
    ColDtypes = pd.DataFrame(df_input.dtypes)
    ColDtypes.reset_index(inplace=True)
    ColDtypes.columns = ['Feature','DataType']
    
    # label Feature a Factor If Less or Equal To nUniqLim
    if AutoClass == False:
        ColDtypes['Factor'] = np.nan
        
        # Identify Number of Unique Values
        ColDtypes['nFactor'] = np.nan
        for row in range(0,len(ColDtypes)):
            ColDtypes.iloc[row,3] = df_input[ColDtypes.iloc[row,0]].nunique()
    else:
        # Flag If Factor
        ColDtypes['Factor'] = np.nan
        for row in range(0,len(ColDtypes)):
            if df_input[ColDtypes.iloc[row,0]].nunique() <= nUniqLim:
                ColDtypes.iloc[row,2] = 1
            else:
                ColDtypes.iloc[row,2] = 0
        # Identify Number of Unique Values
        ColDtypes['nFactor'] = np.nan
        for row in range(0,len(ColDtypes)):
            if ColDtypes.iloc[row,2] == 1:
                ColDtypes.iloc[row,3] = df_input[ColDtypes.iloc[row,0]].nunique()
            else:
                ColDtypes.iloc[row,3] = 'NotFactor'

    # Count Nulls
    ColDtypes['nNulls'] = np.nan
    for row in range(0,len(ColDtypes)):   
        ColDtypes.iloc[row,4] = df_input[ColDtypes.iloc[row,0]].isnull().sum()

    # Null Percent
    ColDtypes['PercentNulls'] = np.nan
    for row in range(0,len(ColDtypes)):
        ColDtypes.iloc[row,5] = round((ColDtypes.iloc[row,4] / len(df_input)) * 100, 1)

    return(ColDtypes)

# Remove Columns With Insignificant Data. Requires target to be at column position 0 and for Metadata to Refreshed Prior to Executing
def JunkColumns(dfinput):
    df_full_metadata = Metadata(dfinput,False,0)
    dfinput = dfinput
    useless_cols = []
    for row in range(0,len(df_full_metadata)):
        if (df_full_metadata.iloc[row,3] in [0,1]) and (df_full_metadata.iloc[row,0] != df_full.columns[0]):
            useless_cols.append(df_full_metadata.iloc[row,0])
        else:
            pass

    dfinput = dfinput.drop(useless_cols,axis=1)

    return(dfinput)

# Correlation Matrix 1
def my_corrmat(df_input):
    sns.set_theme(style="white")

    # Compute the correlation matrix
    corrmat = df_input.corr()

    # Generate a mask for the upper triangle
    mask = np.triu(np.ones_like(corrmat, dtype=bool))

    # Set up the matplotlib figure
    f, ax = plt.subplots(figsize=(11, 9))

    # Generate a custom diverging colormap
    cmap = sns.diverging_palette(230, 20, as_cmap=True)

    # Draw the heatmap with the mask and correct aspect ratio
    sns.heatmap(corrmat, mask=mask, cmap=cmap, center=0,
                square=True, linewidths=.5, cbar_kws={"shrink": .5})
    
    # Correlation Coefficients output
    #corrmat.to_csv(r'Correlation Matrix 2 - r_scores.txt', sep='|', header=True, index=True)
   
# Correlation Matrix 2
    warnings.filterwarnings('ignore')

    def corrdot(*args, **kwargs, df_input):
        corr_r = args[0].corr(args[1], 'pearson')
        corr_text = f"{corr_r:2.2f}".replace("0.", ".")
        ax = plt.gca()
        ax.set_axis_off()
        marker_size = abs(corr_r) * 10000
        ax.scatter([.5], [.5], marker_size, [corr_r], alpha=0.6, cmap="coolwarm",
                   vmin=-1, vmax=1, transform=ax.transAxes)
        font_size = abs(corr_r) * 40 + 5
        ax.annotate(corr_text, [.5, .5,],  xycoords="axes fraction",
                    ha='center', va='center', fontsize=font_size)

    sns.set(style='white', font_scale=1.6)
    g = sns.PairGrid(df_input, aspect=1.4, diag_sharey=False)
    g.map_lower(sns.regplot, lowess=True, ci=False, line_kws={'color': 'black'})
    g.map_diag(sns.distplot, kde_kws={'color': 'black'})
    g.map_upper(corrdot)

# Train Test Splitter
def train_test(df_input,testsize):
    X, y = df_input.iloc[:,1:len(pd.DataFrame(df_input.columns))], df_input.iloc[:,0]
    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=testsize, random_state=1, shuffle=True)
    return X_train, X_test, y_train, y_test

# Quick Preview of Classifiers: Random Forest, SVM, Logit, Decision Tree
def ClassifierBoot():
    classifiers = []

    model1 = RandomForestClassifier()
    classifiers.append(model1)

    model2 = svm.SVC()
    classifiers.append(model2)
    
    model3 = LogisticRegression(max_iter=400)
    classifiers.append(model3)
    
    model4 = tree.DecisionTreeClassifier()
    classifiers.append(model4)

    for clf in classifiers:
        clf.fit(X_train, y_train)
        y_pred= clf.predict(X_test)
        acc = accuracy_score(y_test, y_pred)
        print("Accuracy of %s is %s"%(clf, acc))
        cm = confusion_matrix(y_test, y_pred)

        print("Confusion Matrix of %s is %s"%(clf, cm))
